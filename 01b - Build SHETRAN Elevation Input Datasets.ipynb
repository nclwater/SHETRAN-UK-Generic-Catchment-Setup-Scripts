{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create SHETRAN Elevation Data\n",
    "*Ben Smith | 12/12/2025*\n",
    "\n",
    "This script is designed to take online downloads and reconfigure them into raster layers that can be used to setup SHETRAN models.\n",
    "\n",
    "Todo:\n",
    "- Consider the fixes for the catchments that are below sea level and those that seem to have issues in SHETRAN simulaitons.\n",
    "\n",
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T16:39:23.565480900Z",
     "start_time": "2025-11-27T16:39:21.850223700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rasterio.merge import merge\n",
    "\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import Functions_Model_Set_Up as she\n",
    "\n",
    "root = 'I:/SHETRAN_GB_2021/02_Input_Data/01 - National Data Inputs for SHETRAN UK/'\n",
    "resolution_output = 200\n",
    "\n",
    "# Create a function for simplifying map and table data by removing duplicates:\n",
    "def remove_map_df_duplicates(map_path, table_path, ID_col, duplicate_columns, output_suffix='_unique', data_format: str = '%1.1f'):\n",
    "    \"\"\"\n",
    "    Function to remove duplicate entries in a raster and a linked dataframe.\n",
    "    The duplicates are identified based on the columns in duplicate_columns, and the minimum Raster_ID is used for each group.\n",
    "    ID_col: string of ID column. e.g. 'Raster_ID'\n",
    "    duplicate_columns = list of column srings. e.g. ['Flow Mechanism', 'Summary']\n",
    "    \"\"\"\n",
    "\n",
    "    # Read in the table and map:\n",
    "    print('Reading in data...')\n",
    "    table = pd.read_csv(table_path)\n",
    "    map, mc, mr, mx, my, mcs, mnd, _, _ = she.read_ascii_raster(map_path, data_type=int, replace_NA=False)\n",
    "\n",
    "    # Group using the desired columns and return the raster IDs in each group:\n",
    "    print('Grouping duplicated data...')\n",
    "    groups = table.groupby(duplicate_columns)[ID_col].apply(list).reset_index().Raster_ID\n",
    "\n",
    "    # -- Now run through each group, finding the minimum Raster_ID and changing all other IDs to that in the map.\n",
    "\n",
    "    # Run throug the groups:\n",
    "    print('Running through duplicates...')\n",
    "    for group in groups:\n",
    "        # Find minimum ID: \n",
    "        new_ID = min(group)\n",
    "\n",
    "        # Change the duplicated IDs to the new ID:\n",
    "        for old_ID in group:\n",
    "            ## Table\n",
    "            # table.loc[table[ID_col] == old_ID, ID_col] = new_ID\n",
    "            # Map\n",
    "            map[map == old_ID] = new_ID\n",
    "\n",
    "    # Drop duplicates from the tbale - the method above will change the IDs but will not remove duplicate rows:\n",
    "    print('Dropping duplicates...')\n",
    "    table.drop_duplicates(subset=duplicate_columns, keep='first', inplace=True)#.reset_index(drop=True)\n",
    "\n",
    "    # Reset the indexes so that they run consecutively:\n",
    "    print('resetting indexes...')\n",
    "    counter = 1\n",
    "    for old_ID in sorted(table[ID_col]):\n",
    "        # Table:\n",
    "        table.loc[table[ID_col] == old_ID, ID_col] = counter\n",
    "        # Map\n",
    "        map[map == old_ID] = counter\n",
    "        counter+=1\n",
    "\n",
    "    # Write out the map:\n",
    "    print('Writing data...')\n",
    "    she.write_ascii(array=map, ascii_ouput_path=map_path.replace('.asc', f'{output_suffix}.asc'),\n",
    "        xllcorner=mx, yllcorner=my, cellsize=mcs, ncols=mc, nrows=mr, NODATA_value=mnd, data_format=data_format)\n",
    "\n",
    "    # Remove the duplicated rows from the table and write it to csv:\n",
    "    table.to_csv(table_path.replace('.csv', f'{output_suffix}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Elevation Data\n",
    "\n",
    "Elevation data for the DEM and minDEM is taken from the OS Terrain 50 dataset. This is free to download:\n",
    "https://osdatahub.os.uk/downloads/open/Terrain50\n",
    "\n",
    "Around the coastline, the OS data shows the sea using negative values (presumably taken from a low resolution bathymetry map). It is presumed that this will not impact SHETRAN elevations going forward as the setups do not run to the coast. If much larger negative values were used (i.e. -9999) then this may have a greater impact on those coastal cells compared to the current OS values (0 to -2m or so); although these would still be unlikely to be included within the model domains.\n",
    "\n",
    "This is used to create the DEM and minimum DEM (which is used for rivers).\n",
    "\n",
    "OSNI 50m data for Northern Ireland was downloaded as a csv of points. These were converted into an ascii grid using QGIS:\n",
    " 1. Reprojected from ING to BNG.\n",
    "2. Converted from points to gridded raster with extents rounded to the appropriate 50m.\n",
    "3. No data cells (where there were no points in a raster cell) were filled using Fill No Data, ensuring to only look 1 cell away for a value. This does fill some water cells that should be missing data, but this is non-consequential.\n",
    "4. This filling process was repeated a few times to fill in gaps in the dats where there are lakes etc. Again, non-consequential.\n",
    "5. Data written as an ascii grid for incorporation into the rasters below. You can use QGIS's _Convert Format_ with _Additional command line parameters_ '-co DECIMAL_PRECISION=1' to write this with 1 decimal place to reduce file size.\n",
    "6. The NI data would not immediately merge with the GB data due to an issue with the projection. These were very similar (see below), and so I simply copied a GB projection from a prj file to the NI prj file... I don't think this makes any tangible difference.\n",
    "\n",
    "GB Projection:\n",
    "<code>\n",
    "PROJCS[\"British_National_Grid\",GEOGCS[\"GCS_OSGB_1936\",DATUM[\"D_OSGB_1936\",SPHEROID[\"Airy_1830\",6377563.396,299.3249646]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",400000],PARAMETER[\"False_Northing\",-100000],PARAMETER[\"Central_Meridian\",-2],PARAMETER[\"Scale_Factor\",0.999601272],PARAMETER[\"Latitude_Of_Origin\",49],UNIT[\"Meter\",1]]\n",
    "</code>\n",
    "\n",
    "Original NI Projection\n",
    "<code>\n",
    "PROJCS[\"British_National_Grid\",GEOGCS[\"GCS_OSGB_1936\",DATUM[\"D_OSGB_1936\",SPHEROID[\"Airy_1830\",6377563.396,299.3249646]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",400000.0],PARAMETER[\"False_Northing\",-100000.0],PARAMETER[\"Central_Meridian\",-2.0],PARAMETER[\"Scale_Factor\",0.9996012717],PARAMETER[\"Latitude_Of_Origin\",49.0],UNIT[\"Meter\",1.0]]\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T16:39:30.970266Z",
     "start_time": "2025-11-27T16:39:29.908352500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'I:/SHETRAN_GB_2021/02_Input_Data/01 - National Data Inputs for SHETRAN UK/01 - Downloaded Data\\\\Elevation Data\\\\terr50_gagg_gb/data/'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# The data is within sub-folders, list these:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m OS50_zip_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m01 - Downloaded Data\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mElevation Data\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mterr50_gagg_gb/data/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m OS50_zip_folders \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(OS50_zip_path)\n\u001B[0;32m      4\u001B[0m OS50_zip_folders \u001B[38;5;241m=\u001B[39m [a \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m OS50_zip_folders \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnzipped_data\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m a]\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Setup a new folder to hold the unzipped data:\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: 'I:/SHETRAN_GB_2021/02_Input_Data/01 - National Data Inputs for SHETRAN UK/01 - Downloaded Data\\\\Elevation Data\\\\terr50_gagg_gb/data/'"
     ]
    }
   ],
   "source": [
    "# Unzip the OS download:\n",
    "OS50_zip_path = os.path.join(root, \"01 - Downloaded Data\", \"Elevation Data\", \"terr50_gagg_gb.zip\")  # /data/\n",
    "OS50_unzip_path = os.path.join(os.path.dirname(OS50_zip_path), 'Unzipped_data')\n",
    "with zipfile.ZipFile(OS50_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(os.path.dirname(OS50_zip_path), 'Unzipped_data'))\n",
    "\n",
    "# The data is within sub-folders, list the subfolders (tiles):\n",
    "data_zip_path = os.path.join(os.path.dirname(OS50_zip_path), 'Unzipped_data', 'data')\n",
    "tile_zip_folders = os.listdir(data_zip_path)\n",
    "\n",
    "# Setup a new folder to hold the unzipped data:\n",
    "OS50_unzipped_folder = os.path.join(OS50_unzip_path, 'Collated_data')\n",
    "if not os.path.exists(OS50_unzipped_folder):\n",
    "    os.mkdir(OS50_unzipped_folder)\n",
    "\n",
    "# Unzip the data:\n",
    "for tile_zip_folder in tile_zip_folders:\n",
    "    subtile_folders = os.listdir(os.path.join(data_zip_path, tile_zip_folder))\n",
    "    for zip_folder in subtile_folders:\n",
    "        print(zip_folder)\n",
    "        with zipfile.ZipFile(os.path.join(data_zip_path, tile_zip_folder, zip_folder), 'r') as zip_ref:\n",
    "            zip_ref.extractall(OS50_unzipped_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Join the elevation rasters into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List all .asc files in the folder\n",
    "asc_files = [os.path.join(OS50_unzipped_folder, f) for f in os.listdir(OS50_unzipped_folder) if f.endswith('.asc')]\n",
    "\n",
    "# Open the GB files using rasterio:\n",
    "count = 1\n",
    "raster_list = []\n",
    "for asc_file in asc_files:\n",
    "    print(count, \"/\", len(asc_files))\n",
    "    raster = rasterio.open(asc_file,)\n",
    "    raster_list.append(raster)\n",
    "    count += 1\n",
    "\n",
    "# ---\n",
    "\n",
    "# Open the filled NI file using rasterio:\n",
    "print('NI', \"/\", len(asc_files))\n",
    "raster = rasterio.open(os.path.join(root, 'OSNI_OpenData_50m/OSNI_OpenData_50m_BNG_Filled.asc'),)\n",
    "raster_list.append(raster)\n",
    "\n",
    "# ---\n",
    "\n",
    "# Combine (merge) the rasters:\n",
    "merged_raster, merged_transform = merge(raster_list, nodata=-9999)\n",
    "\n",
    "# Close the opened raster files - you may be able to incorporate this into the loop above.\n",
    "for raster in raster_list:\n",
    "    raster.close()\n",
    "\n",
    "# Extract the first raster band and change 0s to -9999:\n",
    "merged_raster = merged_raster[0]\n",
    "# merged_raster[merged_raster == 0] = -9999  # This was changed to merge(..., nodata=-9999) as it created issues in the fens\n",
    "\n",
    "National_OS50_path = os.path.join(root, 'Processed Data', 'National_OS50.asc')\n",
    "\n",
    "# Write the file as an ascii:\n",
    "she.write_ascii(\n",
    "    array=merged_raster,\n",
    "    ascii_ouput_path=National_OS50_path,\n",
    "    xllcorner=merged_transform[2],\n",
    "    yllcorner=merged_transform[5]-(merged_raster.shape[0]*merged_transform[0]),\n",
    "    cellsize=merged_transform[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Regrid the elevation rasters to the desired size.\n",
    "\n",
    "Note that this does assume that the lower left corner of the national OS50 file is at 0,0, easting northing. Check this if you are redoing this work. you can load the header of the file using the following code:\n",
    "<code>\n",
    "headers = []\n",
    "with open(OS50_zip_path + 'National_OS50.asc', 'r') as fh:\n",
    "for i in range(6):\n",
    "asc_line = fh.readline()\n",
    "headers.append(asc_line.rstrip())\n",
    "headers\n",
    "</code>\n",
    "\n",
    "The first stage of this is to ensure that the 50m data is of the same extent as the 1km data. Rows and columns are added to ensure this. This means that the data has an extent that is in 1km, so can be resampled to divisions of this (1km, 500m, 200m, 100m). This may not work if you try other resolutions as, because the calculations will run from the top left, not the bottom left, the resampled dataset may not have llx/lly coordinates of 0,0. Think about this if you want to use other resolutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'I:/SHETRAN_GB_2021/02_Input_Data/01 - National Data Inputs for SHETRAN UK/Processed Data\\\\National_OS50.asc'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m National_OS50_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mProcessed Data\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNational_OS50.asc\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m national_OS50, _, _, _, _, _, _, _, OS50_header \u001B[38;5;241m=\u001B[39m read_ascii_raster(National_OS50_path, data_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m, replace_NA\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[1], line 60\u001B[0m, in \u001B[0;36mread_ascii_raster\u001B[1;34m(file_path, data_type, return_metadata, replace_NA)\u001B[0m\n\u001B[0;32m     58\u001B[0m headers \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     59\u001B[0m dc \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fh:\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m6\u001B[39m):\n\u001B[0;32m     62\u001B[0m         asc_line \u001B[38;5;241m=\u001B[39m fh\u001B[38;5;241m.\u001B[39mreadline()\n",
      "File \u001B[1;32mc:\\ProgramData\\anaconda3\\envs\\hydrology\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[1;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'I:/SHETRAN_GB_2021/02_Input_Data/01 - National Data Inputs for SHETRAN UK/Processed Data\\\\National_OS50.asc'"
     ]
    }
   ],
   "source": [
    "national_OS50, _, _, _, _, _, _, _, OS50_header = she.read_ascii_raster(National_OS50_path, data_type=float, replace_NA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # If you have not loaded in the dataset (perhaps because you are only testing the code), you can check the dimensions of the 50m dataset using this code:\n",
    "#\n",
    "# OS50_header = {}\n",
    "# with open(OS50_zip_path + 'National_OS50.asc', 'r') as fh:\n",
    "#     for i in range(6):\n",
    "#         asc_line = fh.readline()\n",
    "#         key, val = asc_line.rstrip().split()\n",
    "#         OS50_header[key] = val\n",
    "# OS50_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the national dataset to match existing SHETRAN inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resize the inputs to the desired SHETRAN grid (top right corner should be x: 661000, y: 1241000):\n",
    "row_difference = int(((1241*1000) - float(OS50_header['nrows']) * float(OS50_header['cellsize'])) / float(OS50_header['cellsize']))\n",
    "col_difference = int(((661*1000) - float(OS50_header['ncols']) * float(OS50_header['cellsize'])) / float(OS50_header['cellsize']))\n",
    "\n",
    "if row_difference > 0:\n",
    "    # Create the rows of -9999\n",
    "    new_rows = np.full((row_difference, national_OS50.shape[1]), -9999)\n",
    "    # Add the new rows to the top\n",
    "    national_OS50 = np.vstack((new_rows, national_OS50))\n",
    "\n",
    "# repeat for columns:\n",
    "if row_difference > 0:\n",
    "    new_cols = np.full((national_OS50.shape[0], col_difference), -9999)\n",
    "    national_OS50 = np.hstack((national_OS50, new_cols))  # Remember that these need adding at the end/right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_I have removed the code chuck below as I think it is superfluous. There were some issues resulting from changing 0 values to NA when in fact these are valid elevations. This has been corrected and the code designed to fix/fill the holes left below in case of potential future uses._\n",
    "\n",
    "*it may be of use in the Northern Ireland catchments, where there is a greater presence of NA values over lakes.*\n",
    "\n",
    "_This will fill the holes (na/-9999 values) in the dataset - this code will only fill calls that have a valid value in an adjacent cell._\n",
    "\n",
    "<code>\n",
    "\\# Replace hole_value with NaN for processing\n",
    "raster[raster == -9999] = np.nan\n",
    "\\# Apply the function iteratively\n",
    "filled_national_OS50 = generic_filter(national_OS50, fill_holes, size=3, mode='constant', cval=np.nan)\n",
    "filled_national_OS50[filled_national_OS50 == np.nan] = -9999\n",
    "\\# Write the file as an ascii:\n",
    "write_ascii(\n",
    "    array=filled_national_OS50,\n",
    "    ascii_ouput_path=f'{OS50_zip_path}National_OS50_DEM_preprocessed.asc',\n",
    "    xllcorner=OS50_header['xllcorner'],\n",
    "    yllcorner=OS50_header['yllcorner'],\n",
    "    cellsize=float(OS50_header['cellsize'])\n",
    ")\n",
    "</code>\n",
    "\n",
    "**The following code will give warnings when trying to take the mean of cells that are all np.nan - don't worry, this is doing what it should. (Probably check everything in QGIS or similar at the end though).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the block size for aggregation\n",
    "resolution_input = float(OS50_header['cellsize'])\n",
    "block_size = int(resolution_output/resolution_input)  # For 50m -> 100m, use a block size of 2\n",
    "\n",
    "# Resample using the mean and minimum:\n",
    "DEM = she.cell_reduce_with_pad(national_OS50, block_size, np.mean)\n",
    "minDEM = she.cell_reduce_with_pad(national_OS50, block_size, np.min)\n",
    "\n",
    "# -9999 was converted to np.nan in the loading phase, convert it back\n",
    "DEM[np.isnan(DEM)] = -9999\n",
    "minDEM[np.isnan(minDEM)] = -9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T16:38:00.316230800Z",
     "start_time": "2024-12-16T16:38:00.295432800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the DEM file as an ascii:\n",
    "she.write_ascii(\n",
    "    array=DEM,\n",
    "    ascii_ouput_path=f'{root}/Processed Data/National_OS50_DEM_{resolution_output}m.asc',\n",
    "    xllcorner=OS50_header['xllcorner'],\n",
    "    yllcorner=OS50_header['yllcorner'],\n",
    "    cellsize=resolution_output\n",
    ")\n",
    "\n",
    "# Write the minDEM file as an ascii:\n",
    "she.write_ascii(\n",
    "    array=minDEM,\n",
    "    ascii_ouput_path=f'{root}/Processed Data/National_OS50_minDEM_{resolution_output}m.asc',\n",
    "    xllcorner=OS50_header['xllcorner'],\n",
    "    yllcorner=OS50_header['yllcorner'],\n",
    "    cellsize=resolution_output\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
