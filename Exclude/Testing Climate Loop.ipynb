{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preamble"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# SHETRAN Generic Catchment Simulation Functions\n",
    "# -------------------------------------------------------------\n",
    "# Ben Smith, adapted from previous codes.\n",
    "# 27/07/2022\n",
    "# -------------------------------------------------------------\n",
    "# This code holds the functions required for SHETRAN Generic\n",
    "# Catchment Simulation Creator. Function updates should always\n",
    "# be backwards compatible and as simple as possible to aid future\n",
    "# users.\n",
    "#\n",
    "# Notes:\n",
    "# CHESS rainfall reads in the Y coordinates backwards, if you\n",
    "# change the meteorological inputs then check the coordinates.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- Load in Packages ----------------------------------------\n",
    "import os\n",
    "import itertools\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Create Functions ----------------------------------------\n",
    "def read_ascii_raster(file_path, data_type=int, return_metadata=True):\n",
    "    \"\"\"\n",
    "    Read ascii raster into numpy array, optionally returning headers.\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    dc = {}\n",
    "    with open(file_path, 'r') as fh:\n",
    "        for i in range(6):\n",
    "            asc_line = fh.readline()\n",
    "            headers.append(asc_line.rstrip())\n",
    "            key, val = asc_line.rstrip().split()\n",
    "            dc[key] = val\n",
    "    ncols = int(dc['ncols'])\n",
    "    nrows = int(dc['nrows'])\n",
    "    xll = float(dc['xllcorner'])\n",
    "    yll = float(dc['yllcorner'])\n",
    "    cellsize = float(dc['cellsize'])\n",
    "    nodata = float(dc['NODATA_value'])\n",
    "\n",
    "    arr = np.loadtxt(file_path, dtype=data_type, skiprows=6)\n",
    "\n",
    "    headers = '\\n'.join(headers)\n",
    "    headers = headers.rstrip()\n",
    "\n",
    "    if return_metadata:\n",
    "        return arr, ncols, nrows, xll, yll, cellsize, nodata, headers\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "\n",
    "def get_catchment_coords_ids(xll, yll, urx, ury, cellsize, mask):\n",
    "    \"\"\"Find coordinates of cells in catchment and assign IDs.\"\"\"\n",
    "    x = np.arange(xll, urx + 1, cellsize)\n",
    "    y = np.arange(yll, ury + 1, cellsize)\n",
    "    y[::-1].sort()\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xx_cat = xx[mask == 0]\n",
    "    yy_cat = yy[mask == 0]\n",
    "    cat_coords = []\n",
    "    for xv, yv in zip(xx_cat, yy_cat):\n",
    "        cat_coords.append((yv, xv))\n",
    "\n",
    "    cell_ids = np.zeros(xx.shape, dtype=int) - 9999\n",
    "    counter = 1\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(x)):\n",
    "            if mask[i, j] == 0:\n",
    "                cell_ids[i, j] = counter\n",
    "                counter += 1\n",
    "\n",
    "    return cat_coords, cell_ids\n",
    "\n",
    "\n",
    "def get_date_components(date_string, fmt='%Y-%m-%d'):\n",
    "    date = datetime.datetime.strptime(date_string, fmt)\n",
    "    return date.year, date.month, date.day\n",
    "\n",
    "\n",
    "def get_veg_string(vegetation_array_for_library, static_input_dataset):\n",
    "    \"\"\"\n",
    "    Get string containing vegetation details for the library file.\n",
    "    \"\"\"\n",
    "\n",
    "    veg_vals = [int(v) for v in np.unique(vegetation_array_for_library[vegetation_array_for_library != -9999])]\n",
    "    strickler_dict = {1: 0.6, 2: 3, 3: 0.5, 4: 1, 5: 0.25, 6: 2, 7: 5}\n",
    "\n",
    "    # Extract the vegetation properties from the metadata\n",
    "    veg_props = static_input_dataset.land_cover_lccs.attrs[\"land_cover_key\"].loc[\n",
    "        static_input_dataset.land_cover_lccs.attrs[\"land_cover_key\"][\"Veg Type #\"].isin(veg_vals)].copy()\n",
    "    veg_props[\"strickler\"] = [strickler_dict[item] for item in veg_props[\"Veg Type #\"]]\n",
    "\n",
    "    # Write the subset of properties out to a string\n",
    "    veg_string = veg_props.to_csv(header=False, index=False)\n",
    "    # veg_string = \"<VegetationDetail>\" + veg_string[:-1].replace(\"\\n\", \"</VegetationDetail>\\n<VegetationDetail>\") +\n",
    "    # \"</VegetationDetail>\\n\"\n",
    "    tmp = []\n",
    "    for veg_line in veg_string[:-1].split('\\n'):\n",
    "        tmp.append('<VegetationDetail>' + veg_line.rstrip() + '</VegetationDetail>')\n",
    "    veg_string = '\\n'.join(tmp)\n",
    "    return veg_string\n",
    "\n",
    "\n",
    "def get_soil_strings(orig_soil_types, new_soil_types, static_input_dataset):\n",
    "    \"\"\"\n",
    "    Get the unique soil columns out for the library file.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_soil_types = [int(v) for v in orig_soil_types]\n",
    "    new_soil_types = [int(v) for v in new_soil_types]\n",
    "\n",
    "    # Find the attributes of those columns\n",
    "    soil_props = static_input_dataset.soil_type_APM.attrs[\"soil_key\"].loc[\n",
    "        static_input_dataset.soil_type_APM.attrs[\"soil_key\"][\"Soil Category\"].isin(\n",
    "            orig_soil_types)].copy()  # Change soil_type_APM to soil_type to use old soils!\n",
    "\n",
    "    for orig_type, new_type in zip(orig_soil_types, new_soil_types):\n",
    "        soil_props.loc[soil_props['Soil Category'] == orig_type, 'tmp0'] = new_type\n",
    "    soil_props['Soil Category'] = soil_props['tmp0'].values\n",
    "    soil_props['Soil Category'] = soil_props['Soil Category'].astype(int)\n",
    "\n",
    "    # Rename the soil types for the new format of shetran\n",
    "    soil_props[\"New_Soil_Type\"] = soil_props[\"Soil Type\"].copy()\n",
    "    aquifer_types = [\"NoGroundwater\", \"LowProductivityAquifer\", \"ModeratelyProductiveAquifer\",\n",
    "                     \"HighlyProductiveAquifer\"]\n",
    "\n",
    "    soil_props['tmp1'] = np.where(\n",
    "        (~soil_props['Soil Type'].isin(aquifer_types)),\n",
    "        'Top_' + soil_props['Soil Type'],\n",
    "        soil_props['Soil Type']\n",
    "    )\n",
    "    soil_props['tmp2'] = np.where(\n",
    "        (~soil_props['Soil Type'].isin(aquifer_types)),\n",
    "        'Sub_' + soil_props['Soil Type'],\n",
    "        soil_props['Soil Type']\n",
    "    )\n",
    "    soil_props['New_Soil_Type'] = np.where(\n",
    "        soil_props['Soil Layer'] == 1, soil_props['tmp1'], soil_props['tmp2']\n",
    "    )\n",
    "\n",
    "    # Assign a new soil code to the unique soil types\n",
    "    soil_codes = soil_props.New_Soil_Type.unique()\n",
    "    soil_codes_dict = dict(zip(soil_codes, [i + 1 for i in range(len(soil_codes))]))\n",
    "    soil_props[\"Soil_Type_Code\"] = [soil_codes_dict[item] for item in soil_props.New_Soil_Type]\n",
    "\n",
    "    # Select the relevant information for the library file\n",
    "    soil_types = soil_props.loc[:, [\"Soil_Type_Code\", \"New_Soil_Type\", \"Saturated Water Content\",\n",
    "                                    \"Residual Water Content\", \"Saturated Conductivity (m/day)\",\n",
    "                                    \"vanGenuchten- alpha (cm-1)\", \"vanGenuchten-n\"]]\n",
    "    soil_types.drop_duplicates(inplace=True)\n",
    "\n",
    "    soil_cols = soil_props.loc[:, [\"Soil Category\", \"Soil Layer\", \"Soil_Type_Code\", \"Depth at base of layer (m)\"]]\n",
    "\n",
    "    # Write the subset of properties out to a string\n",
    "    soil_types_string = soil_types.to_csv(header=False, index=False)\n",
    "    soil_cols_string = soil_cols.to_csv(header=False, index=False)\n",
    "\n",
    "    # soil_types_string = \"<SoilProperty>\" + soil_types_string[:-1].replace(\"\\n\", \"</SoilProperty>\\n<SoilProperty>\")\n",
    "    # + \"</SoilProperty>\\n\" soil_cols_string = \"<SoilDetail>\" + soil_cols_string[:-1].replace(\"\\n\",\n",
    "    # \"</SoilDetail>\\n<SoilDetail>\") + \"</SoilDetail>\\n\"\n",
    "\n",
    "    tmp = []\n",
    "    for line in soil_types_string[:-1].split('\\n'):\n",
    "        tmp.append('<SoilProperty>' + line.rstrip() + '</SoilProperty>')\n",
    "    soil_types_string = '\\n'.join(tmp)\n",
    "\n",
    "    tmp = []\n",
    "    for line in soil_cols_string[:-1].split('\\n'):\n",
    "        tmp.append('<SoilDetail>' + line.rstrip() + '</SoilDetail>')\n",
    "    soil_cols_string = '\\n'.join(tmp)\n",
    "\n",
    "    return soil_types_string, soil_cols_string\n",
    "\n",
    "\n",
    "def create_library_file(\n",
    "        sim_output_folder, catch, veg_string, soil_types_string, soil_cols_string,\n",
    "        sim_startime, sim_endtime, prcp_timestep=24, pet_timestep=24):\n",
    "    \"\"\"Create library file.\"\"\"\n",
    "\n",
    "    start_year, start_month, start_day = get_date_components(sim_startime)\n",
    "    end_year, end_month, end_day = get_date_components(sim_endtime)\n",
    "\n",
    "    output_list = [\n",
    "        '<?xml version=1.0?><ShetranInput>',\n",
    "        '<ProjectFile>{}_ProjectFile</ProjectFile>'.format(catch),\n",
    "        '<CatchmentName>{}</CatchmentName>'.format(catch),\n",
    "        '<DEMMeanFileName>{}_DEM.asc</DEMMeanFileName>'.format(catch),\n",
    "        '<DEMminFileName>{}_MinDEM.asc</DEMMinFileName>'.format(catch),\n",
    "        '<MaskFileName>{}_Mask.asc</MaskFileName>'.format(catch),\n",
    "        '<VegMap>{}_LandCover.asc</VegMap>'.format(catch),\n",
    "        '<SoilMap>{}_Soil.asc</SoilMap>'.format(catch),\n",
    "        '<LakeMap>{}_Lake.asc</LakeMap>'.format(catch),\n",
    "        '<PrecipMap>{}_Cells.asc</PrecipMap>'.format(catch),\n",
    "        '<PeMap>{}_Cells.asc</PeMap>'.format(catch),\n",
    "        '<VegetationDetails>',\n",
    "        '<VegetationDetail>Veg Type #, Vegetation Type, Canopy storage capacity (mm), Leaf area index, '\n",
    "        'Maximum rooting depth(m), AE/PE at field capacity,Strickler overland flow coefficient</VegetationDetail>',\n",
    "        veg_string,\n",
    "        '</VegetationDetails>',\n",
    "        '<SoilProperties>',\n",
    "        '<SoilProperty>Soil Number,Soil Type, Saturated Water Content, Residual Water Content, Saturated Conductivity '\n",
    "        '(m/day), vanGenuchten- alpha (cm-1), vanGenuchten-n</SoilProperty> Avoid spaces in the Soil type names',\n",
    "        soil_types_string,\n",
    "        '</SoilProperties>',\n",
    "        '<SoilDetails>',\n",
    "        '<SoilDetail>Soil Category, Soil Layer, Soil Type, Depth at base of layer (m)</SoilDetail>',\n",
    "        soil_cols_string,\n",
    "        '</SoilDetails>',\n",
    "        '<InitialConditions>0</InitialConditions>',\n",
    "        '<PrecipitationTimeSeriesData>{}_Precip.csv</PrecipitationTimeSeriesData>'.format(catch),\n",
    "        '<PrecipitationTimeStep>{}</PrecipitationTimeStep>'.format(prcp_timestep),\n",
    "        '<EvaporationTimeSeriesData>{}_PET.csv</EvaporationTimeSeriesData>'.format(catch),\n",
    "        '<EvaporationTimeStep>{}</EvaporationTimeStep>'.format(pet_timestep),\n",
    "        '<MaxTempTimeSeriesData>{}_Temp.csv</MaxTempTimeSeriesData>'.format(catch),\n",
    "        '<MinTempTimeSeriesData>{}_Temp.csv</MinTempTimeSeriesData>'.format(catch),\n",
    "        '<StartDay>{}</StartDay>'.format(start_day, '02'),\n",
    "        '<StartMonth>{}</StartMonth>'.format(start_month, '02'),\n",
    "        '<StartYear>{}</StartYear>'.format(start_year),\n",
    "        '<EndDay>{}</EndDay>'.format(end_day, '02'),\n",
    "        '<EndMonth>{}</EndMonth>'.format(end_month, '02'),\n",
    "        '<EndYear>{}</EndYear>'.format(end_year),\n",
    "        '<RiverGridSquaresAccumulated>2</RiverGridSquaresAccumulated> Number of upstream grid squares needed to '\n",
    "        'produce a river channel. A larger number will have fewer river channels.',\n",
    "        '<DropFromGridToChannelDepth>2</DropFromGridToChannelDepth> The standard and minimum value is 2 if there are '\n",
    "        'numerical problems with error 1060 this can be increased.',\n",
    "        '<MinimumDropBetweenChannels>0.5</MinimumDropBetweenChannels> This depends on the grid size and how steep the '\n",
    "        'catchment is. A value of 1 is a sensible starting point but more gently sloping catchments it can be reduced.',\n",
    "        '<RegularTimestep>1.0</RegularTimestep> This is the standard Shetran timestep it is automatically reduced in '\n",
    "        'rain. The standard value is 1 hour. The maximum allowed value is 2 hours.',\n",
    "        '<IncreasingTimestep>0.05</IncreasingTimestep> speed of increase in timestep after rainfall back to the '\n",
    "        'standard timestep. The standard value is 0.05. If if there are numerical problems with error 1060 it can be '\n",
    "        'reduced to 0.01 but the simulation will take longer.',\n",
    "        '<SimulatedDischargeTimestep>24.0</SimulatedDischargeTimestep> This should be the same as the measured '\n",
    "        'discharge.',\n",
    "        '<SnowmeltDegreeDayFactor>0.0002</SnowmeltDegreeDayFactor> Units  = mm s-1 C-1',\n",
    "        '</ShetranInput>',\n",
    "    ]\n",
    "    output_string = '\\n'.join(output_list)\n",
    "\n",
    "    f = open(sim_output_folder + catch + \"_LibraryFile.xml\", \"w\")\n",
    "    f.write(output_string)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def create_static_maps(static_input_dataset, xll, yll, ncols, nrows, cellsize,\n",
    "                       static_output_folder, headers, catch, mask, nodata=-9999):\n",
    "    \"\"\"\n",
    "    Write ascii files for DEM, minimum DEM, lake map, vegetation type map and soil map.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper dictionary of details of static fields:\n",
    "    # #- keys are names used for output and values are lists of variable name in\n",
    "    # master static dataset alongside output number format\n",
    "    static_field_details = {\n",
    "        'DEM': ['surface_altitude', '%.2f'],\n",
    "        'MinDEM': ['surface_altitude_min', '%.2f'],\n",
    "        'Lake': ['lake_presence', '%d'],\n",
    "        'LandCover': ['land_cover_lccs', '%d'],\n",
    "        'Soil': ['soil_type_APM', '%d'],\n",
    "    }\n",
    "\n",
    "    xur = xll + (ncols * cellsize) - 1\n",
    "    yur = yll + (nrows * cellsize) - 1\n",
    "\n",
    "    catch_data = static_input_dataset.sel(y=slice(yur, yll), x=slice(xll, xur))\n",
    "\n",
    "    # Save each variable to ascii raster\n",
    "    # Save each variable to ascii raster\n",
    "    for array_name, array_details in static_field_details.items():\n",
    "        array = copy.deepcopy(catch_data[array_details[0]].values)\n",
    "\n",
    "        # Renumber soil types so consecutive from one\n",
    "        if array_name == 'Soil':\n",
    "\n",
    "            array_new = np.zeros(shape=array.shape)\n",
    "            array_new[array_new == 0] = -9999\n",
    "\n",
    "            orig_soil_types = np.unique(array[mask != nodata]).tolist()\n",
    "            new_soil_types = range(1, len(orig_soil_types) + 1)\n",
    "            for orig_type, new_type in zip(orig_soil_types, new_soil_types):\n",
    "                array_new[array == orig_type] = new_type\n",
    "            array = array_new\n",
    "\n",
    "        array[mask == nodata] = nodata\n",
    "        # if array_name in ['DEM', 'MinDEM']:\n",
    "        #     array[mask <= 0] = 0.01\n",
    "\n",
    "        # Remove any values <0 in the DEMs as these will crash the prepare.exe.\n",
    "        if array_name in ['DEM', 'MinDEM']:\n",
    "            array[(array > -9999) & (array <= 0)] = 0.01\n",
    "\n",
    "        # Write the data out:\n",
    "        map_output_path = static_output_folder + catch + '_' + array_name + '.asc'\n",
    "        np.savetxt(\n",
    "            map_output_path, array, fmt=array_details[1], header=headers, comments=''\n",
    "        )\n",
    "\n",
    "        # Get vegetation and soil type arrays for library file construction\n",
    "        if array_name == 'LandCover':\n",
    "            vegetation_arr = array\n",
    "        if array_name == 'Soil':\n",
    "            soil_arr = array\n",
    "\n",
    "    # Also save mask\n",
    "    np.savetxt(\n",
    "        static_output_folder + catch + '_Mask.asc', mask, fmt='%d', header=headers,\n",
    "        comments=''\n",
    "    )\n",
    "\n",
    "    return vegetation_arr, soil_arr, orig_soil_types, new_soil_types\n",
    "\n",
    "def find_rainfall_files(year_from, year_to):\n",
    "    x = [str(y) + '.nc' for y in range(year_from, year_to + 1)]\n",
    "    return x\n",
    "\n",
    "\n",
    "def find_temperature_or_PET_files(folder_path, year_from, year_to):\n",
    "    files = sorted(os.listdir(folder_path))\n",
    "    x = []\n",
    "    for fn in files:\n",
    "        if fn[-3:] == \".nc\":\n",
    "            if int(fn.split('_')[-1][:4]) in range(year_from, year_to + 1):\n",
    "                x.append(fn)\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_climate_data(root_folder, filenames):\n",
    "\n",
    "    first_loop = True\n",
    "\n",
    "    # Run through the different decades, bolting the required catchment data into a common dataframe.\n",
    "    for file in filenames:\n",
    "\n",
    "        print(\"    - \", file)\n",
    "\n",
    "        with xr.open_dataset(os.path.join(root_folder, file)) as DS:\n",
    "\n",
    "            if first_loop:\n",
    "                DS_all_periods = DS\n",
    "                first_loop = False\n",
    "            else:\n",
    "                DS_all_periods = xr.merge([DS_all_periods, DS])\n",
    "\n",
    "    return DS_all_periods\n",
    "\n",
    "# def read_met_data(met_file_path, met_variable_name):\n",
    "#     \"\"\"\n",
    "#     met_variable_name: Probably 'pet' 'tas' 'rainfall_amount'\n",
    "#     \"\"\"\n",
    "#\n",
    "#     met_ds = xr.open_dataset(met_file_path)\n",
    "#\n",
    "#     if met_variable_name in ['tas', 'pet']:\n",
    "#         met_ds = met_ds.drop_vars(['lat', 'lon'])  # Updated from .drop\n",
    "#\n",
    "#     # sometimes pet is called peti, check that here just in case.\n",
    "#     if met_variable_name == 'pet':\n",
    "#         ds_sel_var = list(met_ds.keys())\n",
    "#         met_variable_name = [ds_sel_var[i] for i in np.arange(0, len(ds_sel_var)) if 'pet' in ds_sel_var[i]][0]\n",
    "#\n",
    "#     df = met_ds[met_variable_name].to_dataframe()\n",
    "#\n",
    "#     met_ds.close()\n",
    "#\n",
    "#     df = df.unstack(level=['y', 'x'])\n",
    "#\n",
    "#     y_coords = list(df.columns.levels[1])\n",
    "#     y_coords.sort(reverse=True)\n",
    "#     x_coords = list(df.columns.levels[2])\n",
    "#     x_coords.sort(reverse=False)\n",
    "#\n",
    "#     df_ord = df.loc[:, list(itertools.product([met_variable_name], y_coords, x_coords))]\n",
    "#\n",
    "#     return df_ord\n",
    "\n",
    "\n",
    "def make_series(\n",
    "        met_dataset,\n",
    "        # input_files, input_folder,\n",
    "        xll, yll, urx, ury,\n",
    "        variable, series_startime, series_endtime,\n",
    "        cat_coords, cell_ids, series_output_path, write_cell_id_map=False,\n",
    "        map_output_path=None, map_hdrs=None):\n",
    "    \"\"\"Make and save climate time series for an individual variable.\"\"\"\n",
    "\n",
    "    print(\"-------- Cropping \", variable, \"...\")\n",
    "    if variable == 'rainfall_amount':\n",
    "        ds_sel = met_dataset.sel(y=slice(ury, yll), x=slice(xll, urx))  # Y coords reversed as CHESS lists them backwards\n",
    "    else:\n",
    "        # ds_sel = met_dataset.drop_vars(['lat', 'lon'])  # TODO check whether this works with drop_vars or drop_sel instead of drop\n",
    "        ds_sel = met_dataset.sel(y=slice(yll, ury), x=slice(xll, urx))\n",
    "\n",
    "\n",
    "    # sometimes pet is called peti, check that here just in case.\n",
    "    if variable == 'pet':\n",
    "        ds_sel_var = list(ds_sel.keys())\n",
    "        variable = [ds_sel_var[i] for i in np.arange(0, len(ds_sel_var)) if 'pet' in ds_sel_var[i]][0]\n",
    "\n",
    "    df = ds_sel[variable].to_dataframe()\n",
    "    df = df.unstack(level=['y', 'x'])\n",
    "\n",
    "    y_coords = list(df.columns.levels[1])\n",
    "    y_coords.sort(reverse=True)\n",
    "    x_coords = list(df.columns.levels[2])\n",
    "    x_coords.sort(reverse=False)\n",
    "\n",
    "    dfs = df.loc[:, list(itertools.product([variable], y_coords, x_coords))]\n",
    "\n",
    "    # Subset on time period and cells in catchment\n",
    "    # TODO Check that this doesn't delete data when cut out also check Climate runs.\n",
    "    print(\"---------------- time sub-setting \", variable, \"...\")\n",
    "    df = dfs.sort_index().loc[series_startime:series_endtime]\n",
    "    tmp = np.asarray(df.columns[:])\n",
    "    all_coords = [(y, x) for _, y, x in tmp]\n",
    "    cat_indices = []\n",
    "    ind = 0\n",
    "    for all_pair in all_coords:\n",
    "        if all_pair in cat_coords:\n",
    "            cat_indices.append(ind)\n",
    "        ind += 1\n",
    "    df = df.iloc[:, cat_indices]\n",
    "\n",
    "    # Convert from degK to degC if temperature\n",
    "    if variable == 'tas':\n",
    "        df -= 273.15\n",
    "\n",
    "    # Write outputs\n",
    "    print(\"-------- writing \", variable, \"...\")\n",
    "    headers = np.unique(cell_ids)\n",
    "    headers = headers[headers >= 1]\n",
    "\n",
    "    df.to_csv(series_output_path, index=False, float_format='%.2f', header=headers)\n",
    "    if write_cell_id_map:\n",
    "        np.savetxt(map_output_path, cell_ids, fmt='%d', header=map_hdrs, comments='')\n",
    "\n",
    "\n",
    "def create_climate_files(climate_startime, climate_endtime, mask_path, catch, climate_output_folder,\n",
    "                         prcp_data, tas_data, pet_data):\n",
    "    \"\"\"\n",
    "    Create climate time series.\n",
    "    TODO: Change the method by which the files are sorted. These are currently done by name, which creates an issue\n",
    "     when the later files are alphabetized before the older files (e.g. with PET, where the name changes). It would\n",
    "     be better to use the timestamp from within these files instead.\n",
    "    \"\"\"\n",
    "\n",
    "    start_year, _, _ = get_date_components(climate_startime)\n",
    "    end_year, _, _ = get_date_components(climate_endtime)\n",
    "\n",
    "    # Read catchment mask\n",
    "    mask, ncols, nrows, xll, yll, cellsize, _, hdrs = read_ascii_raster(\n",
    "        mask_path, data_type=int, return_metadata=True\n",
    "    )\n",
    "\n",
    "    # ---\n",
    "    # Precipitation\n",
    "\n",
    "    # Figure out coordinates of upper right\n",
    "    urx = xll + (ncols - 1) * cellsize\n",
    "    ury = yll + (nrows - 1) * cellsize\n",
    "\n",
    "    # Get coordinates and IDs of cells inside catchment\n",
    "    cat_coords, cell_ids = get_catchment_coords_ids(xll, yll, urx, ury, cellsize, mask)\n",
    "\n",
    "    # Make precipitation time series and cell ID map\n",
    "    series_output_path = climate_output_folder + catch + '_Precip.csv'\n",
    "    print(series_output_path)\n",
    "    map_output_path = climate_output_folder + catch + '_Cells.asc'\n",
    "    if not os.path.exists(series_output_path):\n",
    "        make_series(\n",
    "            met_dataset=prcp_data,\n",
    "            xll=xll, yll=yll, urx=urx, ury=ury,\n",
    "            variable='rainfall_amount',\n",
    "            series_startime=climate_startime, series_endtime=climate_endtime,\n",
    "            cat_coords=cat_coords, cell_ids=cell_ids, series_output_path=series_output_path,\n",
    "            write_cell_id_map=True, map_output_path=map_output_path, map_hdrs=hdrs\n",
    "        )\n",
    "\n",
    "    # ---\n",
    "    # Temperature\n",
    "\n",
    "    # Cell centre ll coords\n",
    "    xll_centroid = xll + 500.0\n",
    "    yll_centroid = yll + 500.0\n",
    "\n",
    "    # Figure out coordinates of upper right\n",
    "    urx_centroid = xll_centroid + (ncols - 1) * cellsize\n",
    "    ury_centroid = yll_centroid + (nrows - 1) * cellsize\n",
    "\n",
    "    # Get coordinates and IDs of cells inside catchment\n",
    "    cat_coords_centroid = []\n",
    "    for yv, xv in cat_coords:\n",
    "        cat_coords_centroid.append((yv + 500.0, xv + 500.0))\n",
    "\n",
    "    # Make temperature time series\n",
    "    series_output_path = climate_output_folder + catch + '_Temp.csv'\n",
    "    if not os.path.exists(series_output_path):\n",
    "\n",
    "        make_series(\n",
    "            met_dataset=tas_data,\n",
    "            xll=xll_centroid, yll=yll_centroid, urx=urx_centroid, ury=ury_centroid,\n",
    "            variable='tas',\n",
    "            series_startime=climate_startime, series_endtime=climate_endtime,\n",
    "            cat_coords=cat_coords_centroid, cell_ids=cell_ids, series_output_path=series_output_path\n",
    "        )\n",
    "\n",
    "\n",
    "    # ---\n",
    "    # PET\n",
    "\n",
    "    # Make PET time series\n",
    "    series_output_path = climate_output_folder + catch + '_PET.csv'\n",
    "    if not os.path.exists(series_output_path):\n",
    "\n",
    "        make_series(\n",
    "            met_dataset=pet_data,\n",
    "            xll=xll_centroid, yll=yll_centroid, urx=urx_centroid, ury=ury_centroid,\n",
    "            variable='pet',\n",
    "            series_startime=climate_startime, series_endtime=climate_endtime,\n",
    "            cat_coords=cat_coords_centroid, cell_ids=cell_ids, series_output_path=series_output_path\n",
    "        )\n",
    "\n",
    "\n",
    "def process_catchment(\n",
    "        catch, mask_path, simulation_startime, simulation_endtime, output_subfolder, static_inputs,\n",
    "        produce_climate=True, prcp_data=None, tas_data=None, pet_data=None  # ,q=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create all files needed to run shetran-prepare.\n",
    "    produce_climate is true or false option. If False, climate files will not be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir(output_subfolder):\n",
    "        os.mkdir(output_subfolder)\n",
    "\n",
    "    try:\n",
    "        # Read mask\n",
    "        print(catch, \": reading mask...\")\n",
    "        mask, ncols, nrows, xll, yll, cellsize, _, headers = read_ascii_raster(\n",
    "            mask_path, data_type=int, return_metadata=True)\n",
    "\n",
    "        # Create static maps and return vegetation_array (land cover) and soil arrays/info\n",
    "        print(catch, \": creating static maps...\")\n",
    "        vegetation_array, soil_array, orig_soil_types, new_soil_types = create_static_maps(\n",
    "            static_inputs, xll, yll, ncols, nrows, cellsize, output_subfolder, headers, catch, mask)\n",
    "\n",
    "        # Create climate time series files (and cell ID map)\n",
    "        if produce_climate:\n",
    "            print(catch, \": producing climate files...\")\n",
    "            create_climate_files(simulation_startime, simulation_endtime, mask_path, catch, output_subfolder, prcp_data, tas_data, pet_data)\n",
    "\n",
    "        # Get strings of vegetation and soil properties/details for library file\n",
    "        # print(catch, \": creating vegetation (land use) and soil strings...\")\n",
    "        veg_string = get_veg_string(vegetation_array, static_inputs)\n",
    "        soil_types_string, soil_cols_string = get_soil_strings(orig_soil_types, new_soil_types, static_inputs)\n",
    "\n",
    "        # Create library file\n",
    "        # print(catch, \": creating library file...\")\n",
    "        create_library_file(output_subfolder, catch, veg_string, soil_types_string, soil_cols_string,\n",
    "                            simulation_startime, simulation_endtime)\n",
    "\n",
    "        # sys.exit()\n",
    "\n",
    "    except Exception as E:\n",
    "        print(E.args)\n",
    "        pass\n",
    "\n",
    "\n",
    "def process_mp(mp_catchments, mp_mask_folders, mp_output_folders, mp_simulation_startime,\n",
    "               mp_simulation_endtime, mp_static_inputs, mp_prcp_data, mp_tas_data,\n",
    "               mp_pet_data, mp_produce_climate=False, num_processes=10):\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    # q = manager.Queue()\n",
    "    pool = mp.Pool(num_processes)\n",
    "\n",
    "    jobs = []\n",
    "    for catch in np.arange(0, len(mp_catchments)):\n",
    "        job = pool.apply_async(process_catchment,\n",
    "                               (mp_catchments[catch], mp_mask_folders[catch], mp_simulation_startime,\n",
    "                                mp_simulation_endtime, mp_output_folders[catch], mp_static_inputs, mp_produce_climate,\n",
    "                                mp_prcp_data, mp_tas_data, mp_pet_data))\n",
    "\n",
    "        jobs.append(job)\n",
    "\n",
    "    for job in jobs:\n",
    "        job.get()\n",
    "\n",
    "    # q.put('kill')\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "def read_static_asc_csv(static_input_folder):\n",
    "    \"\"\"\n",
    "    This functions will load in the raw data for the UK, i.e. asc and csv files, and convert these to the dictionary\n",
    "    object used in the setups. There should be 7 files with the following names, all in the same folder (argument):\n",
    "        - SHETRAN_UK_DEM.asc\n",
    "        - SHETRAN_UK_minDEM.asc\n",
    "        - SHETRAN_UK_lake_presence.asc\n",
    "        - SHETRAN_UK_LandCover.asc\n",
    "        - Vegetation_Details.csc\n",
    "        - SHETRAN_UK_SoilGrid_APM.asc\n",
    "        - SHETRAN_UK_SoilDetails.csc\n",
    "    \"\"\"\n",
    "\n",
    "    # Load in the coordinate data (assumes all data has same coordinates:\n",
    "    _, ncols, nrows, xll, yll, cellsize, _, _ = read_ascii_raster(static_input_folder + \"SHETRAN_UK_DEM.asc\",\n",
    "                                                                  return_metadata=True)\n",
    "\n",
    "    # Create eastings and northings. Note, the northings are reversed to match the maps\n",
    "    eastings = np.arange(xll, ncols * cellsize + yll, cellsize)\n",
    "    northings = np.arange(yll, nrows * cellsize + yll, cellsize)[::-1]\n",
    "    eastings_array, northings_array = np.meshgrid(eastings, northings)\n",
    "\n",
    "    ds = xr.Dataset({\n",
    "        \"surface_altitude\": ([\"y\", \"x\"],\n",
    "                             np.loadtxt(static_input_folder + \"SHETRAN_UK_DEM.asc\", skiprows=6),\n",
    "                             {\"units\": \"m\"}),\n",
    "        \"surface_altitude_min\": ([\"y\", \"x\", ],\n",
    "                                 np.loadtxt(static_input_folder + \"SHETRAN_UK_minDEM.asc\", skiprows=6),\n",
    "                                 {\"units\": \"m\"}),\n",
    "        \"lake_presence\": ([\"y\", \"x\"],\n",
    "                          np.loadtxt(static_input_folder + \"SHETRAN_UK_lake_presence.asc\", skiprows=6)),\n",
    "        \"land_cover_lccs\": ([\"y\", \"x\"],\n",
    "                            np.loadtxt(static_input_folder + \"SHETRAN_UK_LandCover.asc\", skiprows=6),\n",
    "                            {\"land_cover_key\": pd.read_csv(static_input_folder + \"Vegetation_Details.csv\")}),\n",
    "        \"soil_type_APM\": ([\"y\", \"x\"],\n",
    "                          np.loadtxt(static_input_folder + \"SHETRAN_UK_SoilGrid_APM.asc\", skiprows=6),\n",
    "                          {\"soil_key\": pd.read_csv(static_input_folder + \"SHETRAN_UK_SoilDetails.csv\")})},\n",
    "        coords={\"easting\": ([\"y\", \"x\"], eastings_array, {\"projection\": \"BNG\"}),\n",
    "                \"northing\": ([\"y\", \"x\"], northings_array, {\"projection\": \"BNG\"}),\n",
    "                \"x\": ([\"x\"], eastings, {\"projection\": \"BNG\"}),\n",
    "                \"y\": ([\"y\"], northings, {\"projection\": \"BNG\"})})\n",
    "\n",
    "    return ds\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -  1999.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990101-19990131.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990201-19990228.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990301-19990331.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990401-19990430.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990501-19990531.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990601-19990630.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990701-19990731.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990801-19990831.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19990901-19990930.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19991001-19991031.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19991101-19991130.nc\n",
      "    -  chess-met_tas_gb_1km_daily_19991201-19991231.nc\n",
      "    -  chess_pet_wwg_199901.nc\n",
      "    -  chess_pet_wwg_199902.nc\n",
      "    -  chess_pet_wwg_199903.nc\n",
      "    -  chess_pet_wwg_199904.nc\n",
      "    -  chess_pet_wwg_199905.nc\n",
      "    -  chess_pet_wwg_199906.nc\n",
      "    -  chess_pet_wwg_199907.nc\n",
      "    -  chess_pet_wwg_199908.nc\n",
      "    -  chess_pet_wwg_199909.nc\n",
      "    -  chess_pet_wwg_199910.nc\n",
      "    -  chess_pet_wwg_199911.nc\n",
      "    -  chess_pet_wwg_199912.nc\n"
     ]
    }
   ],
   "source": [
    "import SHETRAN_GB_Master_Setup_Functions as SF\n",
    "\n",
    "create_climate_data = True\n",
    "rainfall_input_folder = 'I:/CEH-GEAR downloads/'\n",
    "temperature_input_folder = 'I:/CHESS_T/'\n",
    "PET_input_folder = 'I:/CHESS/'\n",
    "\n",
    "start_time = '1999-01-01'\n",
    "end_time = '1999-03-01'\n",
    "\n",
    "# Static Input Data Folder:\n",
    "raw_input_folder = \"I:/SHETRAN_GB_2021/02_Input_Data/Raw ASCII inputs for SHETRAN UK/\"\n",
    "\n",
    "# --- Set Processing Methods -----------------------------------\n",
    "# PYRAMID = 'C:/Users/nbs65/Newcastle University/PYRAMID - General/WP3/02 SHETRAN Simulations/'\n",
    "process_single_catchment = dict(\n",
    "    single=True,\n",
    "    simulation_name='1001',\n",
    "    mask_path=\"I:/SHETRAN_GB_2021/02_Input_Data/1kmBngMasks_Processed/1001_Mask.txt\",\n",
    "    output_folder=\"I:/SHETRAN_GB_2021/04_Historical_Simulations/historical_220601_UK_APM_Additions/QuickloadTest/\")\n",
    "\n",
    "\n",
    "start_year = int(start_time[0:4])\n",
    "end_year = int(end_time[0:4])\n",
    "\n",
    "prcp_input_files = SF.find_rainfall_files(start_year, end_year)\n",
    "tas_input_files = SF.find_temperature_or_PET_files(temperature_input_folder, start_year, end_year)\n",
    "pet_input_files = SF.find_temperature_or_PET_files(PET_input_folder, start_year, end_year)\n",
    "\n",
    "# Read in the Required Datasets:\n",
    "\n",
    "# Rainfall:\n",
    "rainfall_dataset = SF.read_climate_data(root_folder=rainfall_input_folder, filenames=prcp_input_files)\n",
    "\n",
    "# # Temperature:\n",
    "temperature_dataset = SF.read_climate_data(root_folder=temperature_input_folder, filenames=tas_input_files)\n",
    "# temperature_dataset = temperature_dataset.drop_vars(['lat', 'lon'])\n",
    "\n",
    "# # PET:\n",
    "pet_dataset = SF.read_climate_data(root_folder=PET_input_folder, filenames=pet_input_files)\n",
    "# pet_dataset = pet_dataset.drop_vars(['lat', 'lon'])\n",
    "\n",
    "# Soils, Land Cover, Lakes, DEMs:\n",
    "static_data = SF.read_static_asc_csv(raw_input_folder)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 : reading mask...\n",
      "1001 : creating static maps...\n",
      "1001 : producing climate files...\n",
      "-------- Cropping  rainfall_amount ...\n",
      "---------------- time sub-setting  rainfall_amount ...\n",
      "-------- writing  rainfall_amount ...\n",
      "-------- Cropping  tas ...\n",
      "---------------- time sub-setting  tas ...\n",
      "-------- writing  tas ...\n",
      "-------- Cropping  pet ...\n",
      "---------------- time sub-setting  pet ...\n",
      "-------- writing  pet ...\n"
     ]
    }
   ],
   "source": [
    "SF.process_catchment(\n",
    "    catch=process_single_catchment[\"simulation_name\"],\n",
    "    mask_path=process_single_catchment[\"mask_path\"],\n",
    "    simulation_startime=start_time, simulation_endtime=end_time,\n",
    "    output_subfolder=process_single_catchment[\"output_folder\"], static_inputs=static_data,\n",
    "    produce_climate=create_climate_data, prcp_data=rainfall_dataset,\n",
    "    tas_data=temperature_dataset, pet_data=pet_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Change Function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Cropping  pet ...\n",
      "-------- writing  pet ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "catch=process_single_catchment[\"simulation_name\"]\n",
    "mask_path=process_single_catchment[\"mask_path\"]\n",
    "simulation_startime=start_time\n",
    "simulation_endtime=end_time\n",
    "output_subfolder=process_single_catchment[\"output_folder\"]\n",
    "static_inputs=static_data\n",
    "produce_climate=create_climate_data\n",
    "prcp_data=rainfall_dataset\n",
    "tas_data=temperature_dataset\n",
    "pet_data=pet_dataset\n",
    "\n",
    "simulation_startime\n",
    "\n",
    "# -------------- create_climate_files(\n",
    "climate_startime=simulation_startime\n",
    "climate_endtime=simulation_endtime\n",
    "mask_path = mask_path\n",
    "catch = catch\n",
    "climate_output_folder=output_subfolder\n",
    "prcp_data = prcp_data\n",
    "tas_data = tas_data\n",
    "pet_data = pet_data\n",
    "\n",
    "start_year, _, _ = SF.get_date_components(climate_startime)\n",
    "end_year, _, _ = SF.get_date_components(climate_endtime)\n",
    "\n",
    "# Read catchment mask\n",
    "mask, ncols, nrows, xll, yll, cellsize, _, hdrs = SF.read_ascii_raster(\n",
    "    mask_path, data_type=int, return_metadata=True\n",
    ")\n",
    "\n",
    "# ---\n",
    "# Precipitation\n",
    "\n",
    "# Figure out coordinates of upper right\n",
    "urx = xll + (ncols - 1) * cellsize\n",
    "ury = yll + (nrows - 1) * cellsize\n",
    "\n",
    "# Get coordinates and IDs of cells inside catchment\n",
    "cat_coords, cell_ids = SF.get_catchment_coords_ids(xll, yll, urx, ury, cellsize, mask)\n",
    "\n",
    "# Make precipitation time series and cell ID map\n",
    "series_output_path = climate_output_folder + catch + '_Precip.csv'\n",
    "map_output_path = climate_output_folder + catch + '_Cells.asc'\n",
    "if not os.path.exists(series_output_path):\n",
    "    SF.make_series(\n",
    "        met_dataset=prcp_data,\n",
    "        xll=xll, yll=yll, urx=urx, ury=ury,\n",
    "        variable='rainfall_amount',\n",
    "        series_startime=climate_startime, series_endtime=climate_endtime,\n",
    "        cat_coords=cat_coords, cell_ids=cell_ids, series_output_path=series_output_path,\n",
    "        write_cell_id_map=True, map_output_path=map_output_path, map_hdrs=hdrs\n",
    "    )\n",
    "\n",
    "\n",
    "# ---\n",
    "# Temperature\n",
    "\n",
    "# Cell centre ll coords\n",
    "xll_centroid = xll + 500.0\n",
    "yll_centroid = yll + 500.0\n",
    "\n",
    "# Figure out coordinates of upper right\n",
    "urx_centroid = xll_centroid + (ncols - 1) * cellsize\n",
    "ury_centroid = yll_centroid + (nrows - 1) * cellsize\n",
    "\n",
    "# Get coordinates and IDs of cells inside catchment\n",
    "cat_coords_centroid = []\n",
    "for yv, xv in cat_coords:\n",
    "    cat_coords_centroid.append((yv + 500.0, xv + 500.0))\n",
    "\n",
    "# Make temperature time series\n",
    "series_output_path = climate_output_folder + catch + '_PET.csv'\n",
    "met_dataset=pet_data\n",
    "xll=xll_centroid\n",
    "yll=yll_centroid\n",
    "urx=urx_centroid\n",
    "ury=ury_centroid\n",
    "variable='pet'\n",
    "series_startime=climate_startime\n",
    "series_endtime=climate_endtime\n",
    "cat_coords=cat_coords_centroid\n",
    "cell_ids=cell_ids\n",
    "series_output_path=series_output_path\n",
    "\n",
    "print(\"-------- Cropping \", variable, \"...\")\n",
    "if variable == 'rainfall_amount':\n",
    "    ds_sel = met_dataset.sel(y=slice(ury, yll), x=slice(xll, urx))  # Y coords reversed as CHESS lists them backwards\n",
    "else:\n",
    "    # ds_sel = met_dataset.drop_vars(['lat', 'lon'])  # TODO check whether this works with drop_vars or drop_sel instead of drop\n",
    "    ds_sel = met_dataset.sel(y=slice(yll, ury), x=slice(xll, urx))\n",
    "\n",
    "# sometimes pet is called peti, check that here just in case.\n",
    "if variable == 'pet':\n",
    "    ds_sel_var = list(ds_sel.keys())\n",
    "    variable = [ds_sel_var[i] for i in np.arange(0, len(ds_sel_var)) if 'pet' in ds_sel_var[i]][0]\n",
    "\n",
    "df = ds_sel[variable].to_dataframe()\n",
    "df = df.unstack(level=['y', 'x'])\n",
    "\n",
    "y_coords = list(df.columns.levels[1])\n",
    "y_coords.sort(reverse=True)\n",
    "x_coords = list(df.columns.levels[2])\n",
    "x_coords.sort(reverse=False)\n",
    "\n",
    "dfs = df.loc[:, list(itertools.product([variable], y_coords, x_coords))]\n",
    "\n",
    "df = dfs.sort_index().loc[series_startime:series_endtime]\n",
    "tmp = np.asarray(df.columns[:])\n",
    "all_coords = [(y, x) for _, y, x in tmp]\n",
    "cat_indices = []\n",
    "ind = 0\n",
    "for all_pair in all_coords:\n",
    "    if all_pair in cat_coords:\n",
    "        cat_indices.append(ind)\n",
    "    ind += 1\n",
    "df = df.iloc[:, cat_indices]\n",
    "\n",
    "# Convert from degK to degC if temperature\n",
    "if variable == 'tas':\n",
    "    df -= 273.15\n",
    "\n",
    "# Write outputs\n",
    "print(\"-------- writing \", variable, \"...\")\n",
    "headers = np.unique(cell_ids)\n",
    "headers = headers[headers >= 1]\n",
    "\n",
    "df.to_csv(series_output_path, index=False, float_format='%.2f', header=headers)\n",
    "if write_cell_id_map:\n",
    "    np.savetxt(map_output_path, cell_ids, fmt='%d', header=map_hdrs, comments='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "write_cell_id_map=False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}